{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PageRank using Map-Reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task involves implementing the PageRank algorithm using the MapReduce paradigm in Python with the MRJob library. PageRank is an algorithm used by search engines to rank web pages in their search results. It assigns a numerical weight to each element of a hyperlinked set of documents, such as the World Wide Web, with the purpose of measuring its relative importance within the set.\n",
    "\n",
    "The challenge here is to simulate the MapReduce process for PageRank, even without access to a Hadoop cluster. I'll be focusing on understanding the mindset of MapReduce and implementing the algorithm in a smaller scale using Python and MRJob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mrpagerank.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mrpagerank.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import argparse\n",
    "import shutil\n",
    "\n",
    "# node_info = [PageRank, num_outlinks, outlinks] \n",
    "N = 0\n",
    "D = 0\n",
    "\n",
    "# padgett_proc.txt\n",
    "class MRPageRank(MRJob):\n",
    "    def configure_args(self):\n",
    "        super(MRPageRank, self).configure_args()\n",
    "        self.add_passthru_arg('--iter', type=int, default=5, help='Number of iterations')\n",
    "        self.add_passthru_arg('-s', type=float, default=0.85, help='Damping factor')\n",
    "\n",
    "    def ip_mapper(self, _, line):\n",
    "        node_id, node_info = line.split('\\t')\n",
    "        pr, outlinks = eval(node_info)\n",
    "\n",
    "        if len(outlinks) == 0:\n",
    "            yield 1, [pr, line]\n",
    "        else:\n",
    "            yield 1, [0, line]\n",
    "    \n",
    "    def ip_reducer(self, key, values):\n",
    "        global N, D\n",
    "        ips = []\n",
    "        lines = []\n",
    "        N = 0\n",
    "        for value in values:\n",
    "            ips.append(value[0])\n",
    "            lines.append(value[1])\n",
    "            N += 1\n",
    "        \n",
    "        D = sum(ips)\n",
    "\n",
    "        for line in lines:\n",
    "            yield None, line\n",
    "    \n",
    "    def pr_mapper(self, _, line):\n",
    "        node_id, node_info = line.split('\\t')\n",
    "        node_id = int(node_id)\n",
    "        pr, outlinks = eval(node_info)\n",
    "        needed_infos = [node_id, outlinks]\n",
    "\n",
    "        yield node_id, [0.0, needed_infos]\n",
    "        for outlink in outlinks:\n",
    "            yield outlink, [pr / len(outlinks), needed_infos]\n",
    "    \n",
    "    def pr_reducer(self, key, values):\n",
    "        global N, D\n",
    "        prs = []\n",
    "        outlinks = []\n",
    "        for value in values:\n",
    "            pr, infos = value\n",
    "            prs.append(pr)\n",
    "            if infos[0] == key:\n",
    "                outlinks = infos[1]\n",
    "        p = self.options.s\n",
    "        text = str(key) + '\\t' + str([p * sum(prs) + p * D/N + (1.0 - p)/N, outlinks]) + '\\n'\n",
    "        yield None, text\n",
    "    \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.ip_mapper,\n",
    "                   reducer=self.ip_reducer),\n",
    "            MRStep(mapper=self.pr_mapper,\n",
    "                   reducer=self.pr_reducer)\n",
    "        ] * self.options.iter\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Create the argument parser\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Add the arguments\n",
    "    parser.add_argument('filename', type=str, help='the filename')\n",
    "    parser.add_argument('--final-filename', type=str, default='pagerank', help='the number of iterations')\n",
    "    parser.add_argument('--iter', type=int, default=5, help='the number of iterations')\n",
    "    parser.add_argument('-s', type=float, default=0.85, help='the s value')\n",
    "    parser.add_argument('--tollerance', type=float, default=0.00001, help='the tollerance value')\n",
    "\n",
    "    # Parse the command line arguments\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Retrieve the values\n",
    "    filename = args.filename\n",
    "    steps = args.iter\n",
    "    s_value = args.s\n",
    "    tollerance = args.tollerance\n",
    "    final_filename = args.final_filename\n",
    "\n",
    "    shutil.copyfile(filename, final_filename)\n",
    "\n",
    "    prev_pr = {}\n",
    "    new_pr = {}\n",
    "    mr_job = MRPageRank(args=[final_filename, '--iter', str(steps), '-s', str(s_value)])\n",
    "\n",
    "    change = 2\n",
    "    iteration = 1\n",
    "    while change > tollerance:\n",
    "        with mr_job.make_runner() as runner:\n",
    "            runner.run()\n",
    "            change = 0\n",
    "\n",
    "            with open(final_filename, 'w') as file:\n",
    "                for _, line in mr_job.parse_output(runner.cat_output()):\n",
    "                    node_id, info = line.split('\\t')\n",
    "                    pr, outlinks = eval(info)\n",
    "\n",
    "                    if node_id in prev_pr:\n",
    "                        change += abs(prev_pr[node_id] - pr)\n",
    "                    elif iteration == 1:\n",
    "                        change += abs(1/N - pr)\n",
    "\n",
    "                    new_pr[node_id] = pr\n",
    "                    file.write(line)\n",
    "\n",
    "        prev_pr = new_pr\n",
    "        print(f'Iteration: {iteration}')\n",
    "        print(f'Change in l1 norm: {change}')\n",
    "        iteration += steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Short Description of the Solution\n",
    "The provided Python script, mrpagerank.py, implements the PageRank algorithm using the MRJob library for MapReduce simulation:\n",
    "\n",
    "1. **Data Representation**: The script processes input data where each line represents a node (web page) in the graph. Each node is identified by a unique node ID and contains information about its PageRank value and a list of these outbound links.\n",
    "\n",
    "2. **MapReduce Workflow**:\n",
    "\n",
    "- Initialization Phase (ip_mapper and ip_reducer):\n",
    "  - The ip_mapper function parses each line of input data and categorizes nodes into two groups:\n",
    "    - Nodes with no outbound links (no outlinks).\n",
    "    - Nodes with outbound links (outlinks).\n",
    "  - The ip_reducer function receives these categorized nodes, calculates the total number of nodes (`N`), and determines the total dangling PageRank (`D`) from nodes with no outbound links.\n",
    "\n",
    "- PageRank Calculation Phase (pr_mapper and pr_reducer):\n",
    "  - The pr_mapper function takes processed nodes from the initialization phase and calculates the PageRank contribution for each outbound link of a node.\n",
    "  - The pr_reducer function aggregates PageRank contributions received from different nodes and updates the PageRank value for each node using the damping factor (`s`).\n",
    "\n",
    "3. **Iteration and Convergence**:\n",
    "\n",
    "- The steps method defines the MapReduce job sequence to be executed multiple times (iter iterations) to approximate the PageRank values.\n",
    "- The script runs in an iterative loop (while loop) where it:\n",
    "  - Executes the defined MapReduce jobs.\n",
    "  - Calculates the change in PageRank values between iterations to determine convergence (change).\n",
    "  - Updates and writes the PageRank values to the output file (final_filename) after each iteration.\n",
    "\n",
    "4. **Command Line Interface (CLI) Integration**:\n",
    "\n",
    "- The script supports command-line arguments for specifying input file (filename), number of iterations (iter), damping factor (`s`), convergence tolerance (tolerance), and output file (final_filename).\n",
    "- It uses these arguments to configure and execute the MRJob instance accordingly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on Florentine families"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Represents the marriage and business connections among Florentine families during the Renaissance period.\n",
    "\n",
    "Nodes: 16.\n",
    "\n",
    "In my case I have decided to take only the marriage graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "old_file = 'padgett.xml'\n",
    "new_file = 'padgett_proc.txt'\n",
    "\n",
    "# Parse the XML file\n",
    "tree = ET.parse('padgett.xml')\n",
    "root = tree.getroot()\n",
    "\n",
    "# Extract the node IDs\n",
    "node_ids = [node.attrib['id'] for node in root.findall('.//node')]\n",
    "node_ids.sort()\n",
    "id_to_num = {node_id: i for i, node_id in enumerate(node_ids)}\n",
    "num_to_id = {i: node_id for i, node_id in enumerate(node_ids)}\n",
    "n_nodes = len(node_ids)\n",
    "\n",
    "with open(new_file, 'w') as f:\n",
    "    # Extract the network information\n",
    "    network = root.find('.//network[@id=\"PADGM\"]')\n",
    "    node_links = {num_node: [] for num_node in range(len(node_ids))}\n",
    "    for link in network.findall('.//link'):\n",
    "        if int(float(link.attrib['value'])) == 1:\n",
    "            source = id_to_num[link.attrib['source']]\n",
    "            target = id_to_num[link.attrib['target']]\n",
    "            node_links[target].append(source)\n",
    "    \n",
    "    # Calculate the value of 1/N\n",
    "    one_over_n = 1 / n_nodes\n",
    "\n",
    "    for node_id, links in node_links.items():    \n",
    "        # Write the formatted string to the file\n",
    "        f.write(f'{node_id}\\t[{one_over_n}, {links}]\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs specified for inline runner\n",
      "Iteration: 1\n",
      "Change in l1 norm: 0.3750764118360122\n",
      "No configs specified for inline runner\n",
      "Iteration: 6\n",
      "Change in l1 norm: 0.02852020773786826\n",
      "No configs specified for inline runner\n",
      "Iteration: 11\n",
      "Change in l1 norm: 0.0065705498721270895\n",
      "No configs specified for inline runner\n",
      "Iteration: 16\n",
      "Change in l1 norm: 0.001544554205905855\n",
      "No configs specified for inline runner\n",
      "Iteration: 21\n",
      "Change in l1 norm: 0.0003961004680342467\n",
      "No configs specified for inline runner\n",
      "Iteration: 26\n",
      "Change in l1 norm: 9.6453643254623e-05\n",
      "No configs specified for inline runner\n",
      "Iteration: 31\n",
      "Change in l1 norm: 2.4765379896943274e-05\n",
      "No configs specified for inline runner\n",
      "Iteration: 36\n",
      "Change in l1 norm: 6.164122293324248e-06\n"
     ]
    }
   ],
   "source": [
    "!python mrpagerank.py padgett_proc.txt --final-filename=padgett_pagerank.txt --iter=5 -s=0.85 --tollerance=0.00001 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node: 8 - MEDICI - PageRank: 0.1443732044757967\n",
      "Node: 6 - GUADAGNI - PageRank: 0.09742342817750663\n",
      "Node: 14 - STROZZI - PageRank: 0.08722615819345378\n",
      "Node: 1 - ALBIZZI - PageRank: 0.07833903277824031\n",
      "Node: 15 - TORNABUON - PageRank: 0.07057403646625554\n",
      "Node: 12 - RIDOLFI - PageRank: 0.06888543350692938\n",
      "Node: 4 - CASTELLAN - PageRank: 0.0686437086846023\n",
      "Node: 3 - BISCHERI - PageRank: 0.06818004733016528\n",
      "Node: 10 - PERUZZI - PageRank: 0.06720326913312744\n",
      "Node: 13 - SALVIATI - PageRank: 0.06069640315889604\n",
      "Node: 2 - BARBADORI - PageRank: 0.049803017635990134\n",
      "Node: 9 - PAZZI - PageRank: 0.03569682980458627\n",
      "Node: 5 - GINORI - PageRank: 0.03209693961276083\n",
      "Node: 7 - LAMBERTES - PageRank: 0.030603551758192602\n",
      "Node: 0 - ACCIAIUOL - PageRank: 0.030353949184486823\n",
      "Node: 11 - PUCCI - PageRank: 0.009900990099009903\n",
      "\n",
      "Sum of PageRank: 1.0\n"
     ]
    }
   ],
   "source": [
    "node_to_pr = {}\n",
    "with open('padgett_pagerank.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        node_id, node_info = line.split('\\t')\n",
    "        pr, outlinks = eval(node_info)\n",
    "        node_id = int(node_id)\n",
    "        node_to_pr[node_id] = pr\n",
    "\n",
    "sorted_pr = sorted(node_to_pr.items(), key=lambda x: x[1], reverse=True)\n",
    "sum = 0\n",
    "for node_id, pr in sorted_pr:\n",
    "    print(f'Node: {node_id} - {num_to_id[node_id]} - PageRank: {pr}')\n",
    "    sum += pr\n",
    "\n",
    "print()\n",
    "print(f'Sum of PageRank: {sum}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the PageRank values have been calculated for each node in the Florentine families network based on the implemented MapReduce PageRank algorithm. These values represent the relative importance or centrality of each family within the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "\n",
    "- **Medici (Node 8)** has the highest PageRank value, indicating its significant influence within the network.\n",
    "\n",
    "- Other prominent families like **Guadagni (Node 6)**, **Strozzi (Node 14)**, and **Albizzi (Node 1)** also have relatively high PageRank values.\n",
    "\n",
    "- Families with lower PageRank values, such as **Pazzi (Node 9)** and **Pucci (Node 11)**, are less central or influential in the network.\n",
    "\n",
    "These PageRank results provide insights into the structure and importance of relationships among the Florentine families based on their historical connections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wiki-Vote\n",
    "Directed graph (each unordered pair of nodes is saved once): Wiki-Vote.txt \n",
    "\n",
    "Wikipedia voting on promotion to administratorship (till January 2008). Directed edge A->B means user A voted on B becoming Wikipedia administrator.\n",
    "\n",
    "Nodes: 7115 Edges: 103689\n",
    "\n",
    "The PageRank algorithm will be applied to the Wiki-Vote dataset to analyze and rank the importance of users based on their voting behaviors. The goal is to identify the most influential users (nodes) within this network, where influence is determined by the number and quality of votes cast towards promoting other users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_outlinks = {}\n",
    "with open('Wiki-Vote.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        source, target = line.split()\n",
    "        source = int(source)\n",
    "        target = int(target)\n",
    "        if source not in id_to_outlinks:\n",
    "            id_to_outlinks[source] = []\n",
    "        \n",
    "        id_to_outlinks[source].append(target)\n",
    "\n",
    "max_node = max(id_to_outlinks.keys())\n",
    "for i in range(max_node):\n",
    "    if i not in id_to_outlinks:\n",
    "        id_to_outlinks[i] = []\n",
    "\n",
    "n_nodes = len(id_to_outlinks)\n",
    "one_over_n = 1 / n_nodes\n",
    "\n",
    "with open('wiki_proc.txt', 'w') as f:\n",
    "    for node_id, outlinks in id_to_outlinks.items():\n",
    "        f.write(f'{node_id}\\t[{one_over_n}, {outlinks}]\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs specified for inline runner\n",
      "Iteration: 1\n",
      "Change in l1 norm: 0.9165066958253977\n",
      "No configs specified for inline runner\n",
      "Iteration: 4\n",
      "Change in l1 norm: 0.013735677346313988\n",
      "No configs specified for inline runner\n",
      "Iteration: 7\n",
      "Change in l1 norm: 0.0009170520308486114\n",
      "No configs specified for inline runner\n",
      "Iteration: 10\n",
      "Change in l1 norm: 0.00014227941286293005\n",
      "No configs specified for inline runner\n",
      "Iteration: 13\n",
      "Change in l1 norm: 5.8420906086291134e-05\n",
      "No configs specified for inline runner\n",
      "Iteration: 16\n",
      "Change in l1 norm: 3.3918083435145975e-05\n",
      "No configs specified for inline runner\n",
      "Iteration: 19\n",
      "Change in l1 norm: 2.0823232477619004e-05\n",
      "No configs specified for inline runner\n",
      "Iteration: 22\n",
      "Change in l1 norm: 1.2788067646022356e-05\n",
      "No configs specified for inline runner\n",
      "Iteration: 25\n",
      "Change in l1 norm: 7.85347204011188e-06\n",
      "No configs specified for inline runner\n",
      "Iteration: 28\n",
      "Change in l1 norm: 4.823013515728085e-06\n",
      "No configs specified for inline runner\n",
      "Iteration: 31\n",
      "Change in l1 norm: 2.961933173370419e-06\n",
      "No configs specified for inline runner\n",
      "Iteration: 34\n",
      "Change in l1 norm: 1.8189972094222445e-06\n",
      "No configs specified for inline runner\n",
      "Iteration: 37\n",
      "Change in l1 norm: 1.1170916698942601e-06\n",
      "No configs specified for inline runner\n",
      "Iteration: 40\n",
      "Change in l1 norm: 6.860339201735139e-07\n",
      "No configs specified for inline runner\n",
      "Iteration: 43\n",
      "Change in l1 norm: 4.2131058344749815e-07\n",
      "No configs specified for inline runner\n",
      "Iteration: 46\n",
      "Change in l1 norm: 2.587373663205799e-07\n",
      "No configs specified for inline runner\n",
      "Iteration: 49\n",
      "Change in l1 norm: 1.588970859950009e-07\n",
      "No configs specified for inline runner\n",
      "Iteration: 52\n",
      "Change in l1 norm: 9.758266360384389e-08\n"
     ]
    }
   ],
   "source": [
    "!python mrpagerank.py wiki_proc.txt --final-filename=wiki_pagerank.txt --iter=3 -s=0.85 --tollerance=1e-7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the pagerank:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of PageRank: 1.0000001553041662\n"
     ]
    }
   ],
   "source": [
    "sum = 0\n",
    "with open('wiki_pagerank.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        node_id, node_info = line.split('\\t')\n",
    "        pr, outlinks = eval(node_info)\n",
    "        sum += pr\n",
    "\n",
    "print(f'Sum of PageRank: {sum}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web-Google\n",
    "Directed graph (each unordered pair of nodes is saved once): web-Google.txt \n",
    "\n",
    "Webgraph from the Google programming contest, 2002\n",
    "\n",
    "Nodes: 875713 Edges: 5105039\n",
    "\n",
    "The PageRank algorithm will be applied to the Web-Google dataset to analyze and rank the importance of web pages based on their connectivity within the web graph. The goal is to identify the most influential web pages (nodes) that are likely to be important or central in terms of web navigation and information flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_outlinks = {}\n",
    "with open('web-Google.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        source, target = line.split()\n",
    "        source = int(source)\n",
    "        target = int(target)\n",
    "        if source not in id_to_outlinks:\n",
    "            id_to_outlinks[source] = []\n",
    "        \n",
    "        id_to_outlinks[source].append(target)\n",
    "\n",
    "max_node = max(id_to_outlinks.keys())\n",
    "for i in range(max_node):\n",
    "    if i not in id_to_outlinks:\n",
    "        id_to_outlinks[i] = []\n",
    "\n",
    "n_nodes = len(id_to_outlinks)\n",
    "one_over_n = 1 / n_nodes\n",
    "\n",
    "with open('google_proc.txt', 'w') as f:\n",
    "    for node_id, outlinks in id_to_outlinks.items():\n",
    "        f.write(f'{node_id}\\t[{one_over_n}, {outlinks}]\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs specified for inline runner\n",
      "Iteration: 1\n",
      "Change in l1 norm: 0.9916116767044191\n",
      "No configs specified for inline runner\n",
      "Iteration: 11\n",
      "Change in l1 norm: 0.02080242949370202\n",
      "No configs specified for inline runner\n",
      "Iteration: 21\n",
      "Change in l1 norm: 0.0024964243999700923\n",
      "No configs specified for inline runner\n",
      "Iteration: 31\n",
      "Change in l1 norm: 0.0003815088950087467\n",
      "No configs specified for inline runner\n",
      "Iteration: 41\n",
      "Change in l1 norm: 6.366938252637513e-05\n",
      "No configs specified for inline runner\n",
      "Iteration: 51\n",
      "Change in l1 norm: 1.1104888528229216e-05\n",
      "No configs specified for inline runner\n",
      "Iteration: 61\n",
      "Change in l1 norm: 1.9913676943430594e-06\n",
      "No configs specified for inline runner\n",
      "Iteration: 71\n",
      "Change in l1 norm: 3.622847804985308e-07\n",
      "No configs specified for inline runner\n",
      "Iteration: 81\n",
      "Change in l1 norm: 6.655294526715902e-08\n",
      "No configs specified for inline runner\n",
      "Iteration: 91\n",
      "Change in l1 norm: 1.2309390631369617e-08\n",
      "No configs specified for inline runner\n",
      "Iteration: 101\n",
      "Change in l1 norm: 2.2880863013742644e-09\n"
     ]
    }
   ],
   "source": [
    "!python mrpagerank.py google_proc.txt --final-filename=google_pagerank.txt --iter=10 -s=0.85 --tollerance=1e-8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the pagerank:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of PageRank: 0.9999999999967515\n"
     ]
    }
   ],
   "source": [
    "sum = 0\n",
    "with open('google_pagerank.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        node_id, node_info = line.split('\\t')\n",
    "        pr, outlinks = eval(node_info)\n",
    "        sum += pr\n",
    "\n",
    "print(f'Sum of PageRank: {sum}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "The provided implementation demonstrates a simulation of the PageRank algorithm using the MapReduce approach with MRJob in Python.\n",
    "\n",
    "It effectively breaks down the PageRank computation into map \n",
    "and reduce tasks, distributing the workload across multiple nodes.\n",
    "\n",
    "The script supports iterative computation to approximate PageRank values and ensures convergence by monitoring changes in PageRank values between iterations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
